{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "import math\n",
    "import pywt\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configurations\n",
    "hidden1=32\n",
    "second_layer1=32\n",
    "third_layer1=32\n",
    "forth_layer1=16\n",
    "hidden2=32\n",
    "second_layer2=32\n",
    "third_layer2=32\n",
    "forth_layer2=16\n",
    "hidden3=32\n",
    "second_layer3=32\n",
    "third_layer3=32\n",
    "forth_layer3=16\n",
    "hidden4=32\n",
    "second_layer4=32\n",
    "third_layer4=32\n",
    "forth_layer4=16\n",
    "hidden5=32\n",
    "second_layer5=32\n",
    "third_layer5=32\n",
    "forth_layer5=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing averages\n",
    "def yy5(input_data):\n",
    "    A=0\n",
    "    B=0\n",
    "    C=0\n",
    "    D=0\n",
    "    outputavg = []    \n",
    "    for X in input_data:\n",
    "        Y=(X+A+B+C+D)/5\n",
    "        outputavg.append(Y)\n",
    "        D=C\n",
    "        C=B\n",
    "        B=A\n",
    "        A=X\n",
    "\n",
    "    return outputavg\n",
    "\n",
    "#construction of outputs\n",
    "def output(input_data):\n",
    "    out=[]\n",
    "    for i in range(7, len(input_data)-1):\n",
    "        out.append(input_data[i+1])\n",
    "    out = np.append(out, [np.nan])\n",
    "    return out\n",
    "\n",
    "#successive values \n",
    "def successive(successive):\n",
    "   \n",
    "    input_data=[]\n",
    "    for i in range(7, len(successive)):\n",
    "       \n",
    "        input_data.append([successive[i-3]]+[successive[i-2]]+[successive[i-1]]+[successive[i]])\n",
    "    return input_data  \n",
    "\n",
    "#wavelet transform\n",
    "def four_wavelets(training):\n",
    "    input_data=np.array(training)\n",
    "    days = input_data[:,0:4]\n",
    "    \n",
    "    \n",
    "    for row in input_data:\n",
    "            (a, d) = pywt.dwt(days, 'haar')\n",
    "            (a2,d2)=pywt.dwt(a, 'haar') \n",
    "            l3=np.append(a2,d2, axis=1)\n",
    "            l2_3=np.append(l3,d, axis=1)\n",
    "            transformed_df=l2_3\n",
    "    \n",
    "    training=transformed_df\n",
    "    \n",
    "    \n",
    "    return training\n",
    "\n",
    "def train_and_test (input_data):\n",
    "    average=yy5(input_data)\n",
    "    input_data_average=successive(average)\n",
    "    input_data_successive=successive(input_data)\n",
    "    out=output(input_data)\n",
    "\n",
    "\n",
    "#division of data set into training and test data set\n",
    "    split_index=int(len(input_data))-8\n",
    "    N=len(input_data)\n",
    "\n",
    "    input_train=input_data_average[:split_index]\n",
    "    input_test=input_data_average[split_index:]\n",
    "\n",
    "    successive_train=input_data_successive[:split_index]\n",
    "    successive_test=input_data_successive[split_index:]\n",
    "        \n",
    "    second_input_train=successive_train \n",
    "    second_input_test=successive_test \n",
    "\n",
    "    output_train= out[:split_index]\n",
    "    output_test=out[split_index:]\n",
    "\n",
    "    #normalization\n",
    "\n",
    "    inputiavg=np.array(input_train)\n",
    "    inputiavgt=np.array(input_test)\n",
    "\n",
    "    inputsuc=np.array(second_input_train)\n",
    "    inputsuct=np.array(second_input_test)\n",
    "\n",
    "    subtraction_average_train=inputiavg\n",
    "    subtraction_average_test=inputiavgt\n",
    "\n",
    "    subtraction_successive_train=inputsuc\n",
    "    subtraction_successive_test=inputsuct\n",
    "\n",
    "    subtraction_average_train=subtraction_average_train.sum(axis=1)/4\n",
    "    subtraction_average_test=subtraction_average_test.sum(axis=1)/4\n",
    "\n",
    "    subtraction_successive_train=subtraction_successive_train.sum(axis=1)/4\n",
    "    subtraction_successive_test=subtraction_successive_test.sum(axis=1)/4\n",
    "\n",
    "    #normalization of inputs\n",
    "    first_input_train=input_train-subtraction_average_train[:, None]\n",
    "    first_input_test=input_test-subtraction_average_test[:,None]\n",
    "\n",
    "    output_train=output_train-subtraction_successive_train\n",
    "    output_test=output_test-subtraction_successive_test\n",
    "\n",
    "    second_input_train=second_input_train-subtraction_successive_train[:,None]\n",
    "    second_input_test=second_input_test-subtraction_successive_test[:,None]\n",
    "\n",
    "    #4inputs WT\n",
    "    final_first_w_input_train=four_wavelets(first_input_train)\n",
    "\n",
    "    X_train=np.array(final_first_w_input_train[:, 1:])\n",
    "    y_train=np.array(output_train)\n",
    "\n",
    "    m_primary=len(X_train[0,:])\n",
    "    p_primary=np.size(y_train[0])\n",
    "    N_primary=len(X_train)\n",
    "\n",
    "    model= Sequential ([\n",
    "        Dense(hidden1, input_dim=m_primary, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_primary)\n",
    "        ])\n",
    "        \n",
    "    sgd=SGD(lr=0.05,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_absolute_error','mean_squared_logarithmic_error','cosine_similarity','logcosh'])\n",
    "    history1=model.fit(X_train, y_train, batch_size=N_primary, epochs=300, shuffle=False, verbose=0)  \n",
    "\n",
    "    predicted_train = model.predict(X_train) \n",
    "    predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "    error_train1=predicted_train-y_train\n",
    "\n",
    "    error_train=pd.DataFrame(error_train1)\n",
    "    add_train=four_wavelets(second_input_train) \n",
    "    \n",
    "    X_error_train1=np.array(add_train[:, 1:])\n",
    "    y_error_train1=np.array(error_train)\n",
    "\n",
    "    m_second=len(X_error_train1[0,:])\n",
    "    p_second=np.size(y_train[0])\n",
    "    N_second=len(X_error_train1)\n",
    "\n",
    "    error_model1= Sequential ([\n",
    "        Dense(hidden2, input_dim=m_second, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_second)\n",
    "    ])\n",
    "\n",
    "    sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model1.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "    history3=error_model1.fit(X_error_train1, y_error_train1, batch_size=N_second, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    error_predicted_tr = error_model1.predict(X_error_train1)\n",
    "    error_predicted_tr = np.reshape(error_predicted_tr, (error_predicted_tr.size,))\n",
    "\n",
    "    compensated1_train=(predicted_train+subtraction_successive_train)-(error_predicted_tr)\n",
    "\n",
    "    error_train2a=compensated1_train-(y_train+subtraction_successive_train)\n",
    "\n",
    "    error_train2=pd.DataFrame(error_train2a)\n",
    "    error_train2 [1]= error_train2[0].shift(1)\n",
    "    error_train2 [2]=error_train2[1].shift(1)\n",
    "    error_train2 [3]=error_train2[2].shift(1)\n",
    "    error_train2[4]=error_train2[3].shift(1)\n",
    "    error_train2 = error_train2.replace(np.nan, 0)\n",
    "\n",
    "    ##error normalization\n",
    "    subtraction_error_train2=np.array(error_train2)\n",
    "    subtraction_error_train2=subtraction_error_train2[:,:-1]\n",
    "    subtraction_error_train2=subtraction_error_train2.sum(axis=1)/4\n",
    "\n",
    "    error_train2=error_train2-subtraction_error_train2[:, None]\n",
    "\n",
    "    error_train2=np.array(error_train2)\n",
    "    days_train = error_train2[:,1:5]\n",
    "    input3_train=four_wavelets(days_train)\n",
    "    output3_train=error_train2[:,0:1]\n",
    "\n",
    "    X_error_train2=np.array(input3_train[:, 1:])\n",
    "    y_error_train2=np.array(output3_train)\n",
    "\n",
    "    #####3rd NN\n",
    "    m_error=len(X_error_train2[0,:])\n",
    "    p_error=np.size(y_error_train2[0])\n",
    "    N_error=len(X_error_train2)\n",
    "\n",
    "    error_model2= Sequential ([\n",
    "        Dense(hidden3, input_dim=m_error, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_error)\n",
    "    ])\n",
    "    \n",
    "    sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model2.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "    history4=error_model2.fit(X_error_train2, y_error_train2, batch_size=N_error, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "    error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "\n",
    "    compensated_y_train=compensated1_train-(error_predicted_tr2+subtraction_error_train2)\n",
    "\n",
    "    error_predicted_tr3=error_predicted_tr2+subtraction_error_train2\n",
    "\n",
    "    training_final_add=np.column_stack((predicted_train, error_predicted_tr))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "\n",
    "    ####final NN\n",
    "    m_final=len(training_final_add[0,:])\n",
    "    p_final=np.size(y_train[0])\n",
    "    N_final=len(training_final_add)\n",
    "\n",
    "    final_model= Sequential ([\n",
    "        Dense(hidden4, input_dim=m_final, activation='relu'), \n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(second_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(third_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(forth_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "        Dense(p_final)\n",
    "    ])\n",
    "\n",
    "    sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    final_model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "    final_history=final_model.fit(training_final_add, y_train, batch_size=N_final, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    final_predicted_tr =final_model.predict(training_final_add)\n",
    "    final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "\n",
    "    final_first_w_input_test=four_wavelets(first_input_test)\n",
    "\n",
    "    X_test=np.array(final_first_w_input_test[:,1:])\n",
    "    y_test=np.array(output_test)\n",
    "\n",
    "\n",
    "    predicted_test = model.predict(X_test) \n",
    "    predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "    error_test1=predicted_test-y_test\n",
    "\n",
    "\n",
    "    error_test=pd.DataFrame(error_test1)\n",
    "    add_test=four_wavelets(second_input_test) \n",
    "    \n",
    "    X_error_test1=np.array(add_test[:, 1:])\n",
    "\n",
    "    error_predicted_tes = error_model1.predict(X_error_test1)\n",
    "    error_predicted_tes = np.reshape(error_predicted_tes, (error_predicted_tes.size,))\n",
    "\n",
    "    compensated1_test=(predicted_test+subtraction_successive_test)-(error_predicted_tes)\n",
    "\n",
    "    error_test2a=compensated1_test-(y_test+subtraction_successive_test)\n",
    "\n",
    "    error_test2=pd.DataFrame(error_test2a)\n",
    "    error_test2 [1]= error_test2[0].shift(1)\n",
    "    error_test2 [2]=error_test2[1].shift(1)\n",
    "    error_test2 [3]=error_test2[2].shift(1)\n",
    "    error_test2[4]=error_test2[3].shift(1)\n",
    "    error_test2 = error_test2.replace(np.nan, 0)\n",
    "\n",
    "    subtraction_error_test2=np.array(error_test2)\n",
    "    subtraction_error_test2=subtraction_error_test2[:,:-1]\n",
    "    subtraction_error_test2=subtraction_error_test2.sum(axis=1)/4\n",
    "\n",
    "    error_test2=error_test2-subtraction_error_test2[:,None]\n",
    "\n",
    "    error_test2=np.array(error_test2)\n",
    "    days_test = error_test2[:,1:5]\n",
    "    input3_test=four_wavelets(days_test)\n",
    "    output3_test=error_test2[:,0:1]\n",
    "\n",
    "    X_error_test2=np.array(input3_test[:, 1:])\n",
    "\n",
    "    error_predicted_tes2 = error_model2.predict( X_error_test2)\n",
    "    error_predicted_tes2= np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "    compensated_y_test=compensated1_test-(error_predicted_tes2+subtraction_error_test2)\n",
    "\n",
    "    error_predicted_tes3=error_predicted_tes2+subtraction_error_test2\n",
    "\n",
    "    test_final_add=np.column_stack((predicted_test, error_predicted_tes))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "\n",
    "    final_predicted_test = final_model.predict(test_final_add)\n",
    "    final_predicted_test = np.reshape(final_predicted_test, (final_predicted_test.size,))\n",
    "\n",
    "    y_test=y_test+subtraction_successive_test\n",
    "    \n",
    "    final_y_test=final_predicted_test+subtraction_successive_test\n",
    "    y_test = np.reshape(y_test, (y_test.size,))\n",
    "    final_y_test = np.reshape(final_y_test, (final_y_test.size,))\n",
    "\n",
    "    return final_y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c5dcfd21eb0a78f90305e0fea3eb6f09987102a804f4bba64688b0b6743d9b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
